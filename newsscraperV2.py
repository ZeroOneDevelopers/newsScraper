import logging
import requests
from bs4 import BeautifulSoup
import pandas as pd
import streamlit as st
from textblob import TextBlob
import textstat
from collections import Counter
from urllib.parse import urljoin, urlparse
import nltk

# ----------------------------
# Î¡ÏÎ¸Î¼Î¹ÏƒÎ· NLTK Î³Î¹Î± stopwords
# ----------------------------
try:
    from nltk.corpus import stopwords
    english_stopwords = set(stopwords.words('english'))
except LookupError:
    nltk.download('stopwords')
    from nltk.corpus import stopwords
    english_stopwords = set(stopwords.words('english'))

# ----------------------------
# Î¡ÏÎ¸Î¼Î¹ÏƒÎ· Logging
# ----------------------------
logging.basicConfig(level=logging.INFO, 
                    format='%(asctime)s - %(levelname)s - %(message)s')

# ----------------------------
# Î¡Ï…Î¸Î¼Î¯ÏƒÎµÎ¹Ï‚ Site & ÎšÎ±Ï„Î·Î³Î¿ÏÎ¹ÏÎ½
# ----------------------------
sites = [
    {
        "name": "ABC News",
        "categories": {
            "Politics": "https://abcnews.go.com/Politics",
            "Business": "https://abcnews.go.com/Business",
            "Tech": "https://abcnews.go.com/Technology"
        }
    }
]

# ----------------------------
# Î£Ï…Î½Î¬ÏÏ„Î·ÏƒÎ· ÎšÎ±Î¸Î±ÏÎ¹ÏƒÎ¼Î¿Ï URL
# ----------------------------
def clean_url(base_url: str, link: str) -> str:
    """
    Î£Ï…Î½Î´Ï…Î¬Î¶ÎµÎ¹ Ï„Î¿ base_url Î¼Îµ Î­Î½Î± ÏƒÏ‡ÎµÏ„Î¹ÎºÏŒ link Ï‡ÏÎ·ÏƒÎ¹Î¼Î¿Ï€Î¿Î¹ÏÎ½Ï„Î±Ï‚ Ï„Î¿ urljoin.
    """
    if not link:
        return None
    return link if link.startswith("http") else urljoin(base_url, link)

# ----------------------------
# Î£Ï…Î½Î¬ÏÏ„Î·ÏƒÎ· Scraping Î†ÏÎ¸ÏÏ‰Î½
# ----------------------------
def scrape_articles(url: str, title_selector: str, link_selector: str, base_url: str = "") -> list:
    """
    Î‘Î½Ï„Î»ÎµÎ¯ Ï„Î¯Ï„Î»Î¿Ï…Ï‚ ÎºÎ±Î¹ URL Î¬ÏÎ¸ÏÏ‰Î½ Î±Ï€ÏŒ Ï„Î· ÏƒÎµÎ»Î¯Î´Î± Ï€Î¿Ï… Î´Î¯Î½ÎµÏ„Î±Î¹, Ï‡ÏÎ·ÏƒÎ¹Î¼Î¿Ï€Î¿Î¹ÏÎ½Ï„Î±Ï‚ Ï„Î¿Ï…Ï‚ Î±Î½Ï„Î¯ÏƒÏ„Î¿Î¹Ï‡Î¿Ï…Ï‚ CSS selectors.
    Î•Ï€Î¹ÏƒÏ„ÏÎ­Ï†ÎµÎ¹ Î¼Î¹Î± Î»Î¯ÏƒÏ„Î± Î¼Îµ tuples (title, link).
    """
    headers = {"User-Agent": "Mozilla/5.0"}
    try:
        response = requests.get(url, headers=headers, timeout=10)
        response.raise_for_status()
        soup = BeautifulSoup(response.text, 'html.parser')

        # Î‘Î½Î¬ÎºÏ„Î·ÏƒÎ· Ï„Î¯Ï„Î»Ï‰Î½
        titles = [elem.get_text(strip=True) for elem in soup.select(title_selector)][:10]

        # Î‘Î½Î¬ÎºÏ„Î·ÏƒÎ· links
        links = [clean_url(base_url, elem.get('href')) for elem in soup.select(link_selector)][:10]

        # Î•Î¾Î±ÏƒÏ†Î±Î»Î¯Î¶Î¿Ï…Î¼Îµ ÏŒÏ„Î¹ Î¿ Î±ÏÎ¹Î¸Î¼ÏŒÏ‚ Ï„Ï‰Î½ links ÎµÎ¯Î½Î±Î¹ Î¯Î´Î¹Î¿Ï‚ Î¼Îµ Ï„Ï‰Î½ Ï„Î¯Ï„Î»Ï‰Î½
        if len(links) < len(titles):
            links.extend([None] * (len(titles) - len(links)))

        articles = [(title, link) for title, link in zip(titles, links) if title]
        logging.info(f"Scraped {len(articles)} articles from {url}")
        return articles

    except requests.exceptions.RequestException as e:
        logging.error(f"Request error on {url}: {e}")
        return []

# ----------------------------
# Î£Ï…Î½Î¬ÏÏ„Î·ÏƒÎ· Î›Î®ÏˆÎ·Ï‚ Î ÎµÏÎ¹ÎµÏ‡Î¿Î¼Î­Î½Î¿Ï… ÎºÎ±Î¹ SEO Data
# ----------------------------
def fetch_article_content(url: str) -> tuple:
    """
    Î‘Î½Ï„Î»ÎµÎ¯ Ï„Î¿ Ï€ÎµÏÎ¹ÎµÏ‡ÏŒÎ¼ÎµÎ½Î¿ Ï„Î¿Ï… Î¬ÏÎ¸ÏÎ¿Ï… ÎºÎ±Î¹ Ï„Î± SEO ÏƒÏ„Î¿Î¹Ï‡ÎµÎ¯Î± (meta title, meta description, meta keywords)
    Î±Ï€ÏŒ Ï„Î¿ Î´Î¿ÏƒÎ¼Î­Î½Î¿ URL.
    Î•Ï€Î¹ÏƒÏ„ÏÎ­Ï†ÎµÎ¹ tuple: (content, meta_title, meta_description, meta_keywords)
    """
    if not url:
        return "No URL provided.", None, None, None

    headers = {"User-Agent": "Mozilla/5.0"}
    try:
        response = requests.get(url, headers=headers, timeout=10)
        response.raise_for_status()
        soup = BeautifulSoup(response.text, 'html.parser')

        # Î ÏÎ¿ÏƒÏ€Î¬Î¸ÎµÎ¹Î± ÎµÎ¾Î±Î³Ï‰Î³Î®Ï‚ Ï„Î¿Ï… Ï€ÎµÏÎ¹ÎµÏ‡Î¿Î¼Î­Î½Î¿Ï… Î±Ï€ÏŒ Ï„Î¿ <article> tag, ÎµÎ¬Î½ Ï…Ï€Î¬ÏÏ‡ÎµÎ¹
        article_tag = soup.find('article')
        if article_tag:
            paragraphs = article_tag.find_all('p')
        else:
            paragraphs = soup.find_all('p')

        content = " ".join([p.get_text(strip=True) for p in paragraphs if len(p.get_text(strip=True)) > 50])
        if not content:
            content = "No relevant content found."

        # Î•Î¾Î±Î³Ï‰Î³Î® SEO ÏƒÏ„Î¿Î¹Ï‡ÎµÎ¯Ï‰Î½
        meta_title_tag = soup.find("title")
        meta_title = meta_title_tag.get_text(strip=True) if meta_title_tag else "No Title"
        meta_desc_tag = soup.find("meta", attrs={"name": "description"})
        meta_description = meta_desc_tag["content"] if meta_desc_tag and meta_desc_tag.get("content") else "No Description"
        meta_keywords_tag = soup.find("meta", attrs={"name": "keywords"})
        meta_keywords = meta_keywords_tag["content"] if meta_keywords_tag and meta_keywords_tag.get("content") else "No Keywords"

        return content, meta_title, meta_description, meta_keywords

    except requests.exceptions.Timeout:
        logging.error(f"Timeout error when fetching {url}")
        return "Error: Request timed out.", None, None, None
    except Exception as e:
        logging.error(f"Error fetching content from {url}: {e}")
        return f"Error fetching content: {e}", None, None, None

# ----------------------------
# Î£Ï…Î½Î¬ÏÏ„Î·ÏƒÎ· Î‘Î½Î¬Î»Ï…ÏƒÎ·Ï‚ Î£Ï…Î½Î±Î¹ÏƒÎ¸Î®Î¼Î±Ï„Î¿Ï‚
# ----------------------------
def analyze_sentiment(text: str) -> float:
    """
    Î‘Î½Î±Î»ÏÎµÎ¹ Ï„Î¿ ÏƒÏ…Î½Î±Î¯ÏƒÎ¸Î·Î¼Î± Ï„Î¿Ï… ÎºÎµÎ¹Î¼Î­Î½Î¿Ï… Ï‡ÏÎ·ÏƒÎ¹Î¼Î¿Ï€Î¿Î¹ÏÎ½Ï„Î±Ï‚ Ï„Î¿ TextBlob.
    Î•Ï€Î¹ÏƒÏ„ÏÎ­Ï†ÎµÎ¹ Ï„Î·Î½ polarity Ï‰Ï‚ float Î¼Îµ ÏƒÏ„ÏÎ¿Î³Î³Ï…Î»Î¿Ï€Î¿Î¯Î·ÏƒÎ· ÏƒÎµ 3 Î´ÎµÎºÎ±Î´Î¹ÎºÎ¬.
    """
    if not text or text.startswith("Error"):
        return 0.0
    blob = TextBlob(text)
    return round(blob.sentiment.polarity, 3)

# ----------------------------
# Î£Ï…Î½Î¬ÏÏ„Î·ÏƒÎ· Î‘Î½Î¬Î»Ï…ÏƒÎ·Ï‚ SEO
# ----------------------------
def analyze_seo(text: str) -> tuple:
    """
    Î¥Ï€Î¿Î»Î¿Î³Î¯Î¶ÎµÎ¹ Ï„Î·Î½ Ï€Ï…ÎºÎ½ÏŒÏ„Î·Ï„Î± Î»Î­Î¾ÎµÏ‰Î½ (keyword density) Î±Ï†Î±Î¹ÏÏÎ½Ï„Î±Ï‚ Ï„Î± ÎºÎ¿Î¹Î½Î¬ stopwords,
    ÎºÎ±Î¹ Ï…Ï€Î¿Î»Î¿Î³Î¯Î¶ÎµÎ¹ Ï„Î¹Ï‚ Î¼ÎµÏ„ÏÎ¹ÎºÎ­Ï‚ Î±Î½Î±Î³Î½Ï‰ÏƒÎ¹Î¼ÏŒÏ„Î·Ï„Î±Ï‚ (Flesch Reading Ease ÎºÎ±Î¹ Flesch-Kincaid Grade).
    Î•Ï€Î¹ÏƒÏ„ÏÎ­Ï†ÎµÎ¹ tuple: (keyword_density, readability_scores)
    """
    if not text:
        return {}, {"flesch_reading_ease": "N/A", "flesch_kincaid_grade": "N/A"}

    words = text.split()
    word_count = len(words)
    if word_count == 0:
        return {}, {"flesch_reading_ease": "N/A", "flesch_kincaid_grade": "N/A"}

    # ÎšÎ±Î¸Î±ÏÎ¹ÏƒÎ¼ÏŒÏ‚ Î»Î­Î¾ÎµÏ‰Î½ (Î±Ï†Î±Î¯ÏÎµÏƒÎ· ÏƒÎ·Î¼ÎµÎ¯Ï‰Î½ ÏƒÏ„Î¯Î¾Î·Ï‚ ÎºÎ±Î¹ Î¼ÎµÏ„Î±Ï„ÏÎ¿Ï€Î® ÏƒÎµ lowercase)
    cleaned_words = [word.strip(".,!?;:()[]\"'").lower() for word in words]
    # Î‘Ï†Î±Î¯ÏÎµÏƒÎ· stopwords
    filtered_words = [word for word in cleaned_words if word and word not in english_stopwords]
    
    word_freq = Counter(filtered_words)
    keyword_density = {word: round((count / word_count) * 100, 2)
                       for word, count in word_freq.items() if count > 2}

    readability_scores = {
        "flesch_reading_ease": textstat.flesch_reading_ease(text),
        "flesch_kincaid_grade": textstat.flesch_kincaid_grade(text)
    }
    
    return keyword_density, readability_scores

# ----------------------------
# Î£Ï…Î½Î¬ÏÏ„Î·ÏƒÎ· Î”Î·Î¼Î¹Î¿Ï…ÏÎ³Î¯Î±Ï‚ DataFrame
# ----------------------------
def build_dataframe(data: list) -> pd.DataFrame:
    """
    Î”Î·Î¼Î¹Î¿Ï…ÏÎ³ÎµÎ¯ Î­Î½Î± DataFrame Ï„Î·Ï‚ Pandas Î±Ï€ÏŒ Ï„Î± ÏƒÏ…Î»Î»ÎµÎ³Î¼Î­Î½Î± Î´ÎµÎ´Î¿Î¼Î­Î½Î±.
    """
    return pd.DataFrame(data)

# ----------------------------
# Î£Ï…Î½Î¬ÏÏ„Î·ÏƒÎ· Î•Î¾Î±Î³Ï‰Î³Î®Ï‚ ÏƒÎµ CSV
# ----------------------------
def download_csv(df: pd.DataFrame) -> bytes:
    """
    ÎœÎµÏ„Î±Ï„ÏÎ­Ï€ÎµÎ¹ Ï„Î¿ DataFrame ÏƒÎµ CSV Î³Î¹Î± ÎµÎ¾Î±Î³Ï‰Î³Î®.
    """
    return df.to_csv(index=False).encode('utf-8')

# ----------------------------
# ÎšÏÏÎ¹Î± Î£Ï…Î½Î¬ÏÏ„Î·ÏƒÎ·
# ----------------------------
def main():
    # Î¡Ï…Î¸Î¼Î¯ÏƒÎµÎ¹Ï‚ Streamlit
    st.set_page_config(page_title="ABC News SEO & Sentiment Dashboard", layout="wide")
    st.title("ğŸ“Š ABC News SEO & Sentiment Dashboard")

    data = []
    site = sites[0]  # Î¥Ï€Î¿Î¸Î­Ï„Î¿Ï…Î¼Îµ ÏŒÏ„Î¹ Î­Ï‡Î¿Ï…Î¼Îµ Î¼ÏŒÎ½Î¿ Î­Î½Î± site (ABC News)

    # ÎŸÏÎ¹ÏƒÎ¼ÏŒÏ‚ CSS selectors Î³Î¹Î± ÎºÎ¬Î¸Îµ ÎºÎ±Ï„Î·Î³Î¿ÏÎ¯Î±
    selectors = {
        "Politics": {
            "title": "h2 a.AnchorLink",
            "link": "h2 a.AnchorLink"
        },
        "Business": {
            "title": "h2.News__Item__Headline, h4.News__title",
            "link": "h2.News__Item__Headline a, h4.News__title a"
        },
        "Tech": {
            "title": "h2 a.AnchorLink",
            "link": "h2 a.AnchorLink"
        }
    }

    # Sidebar Ï†Î¯Î»Ï„ÏÎ±: ÎµÏ€Î¹Î»Î¿Î³Î® ÎºÎ±Ï„Î·Î³Î¿ÏÎ¯Î±Ï‚ ÎºÎ±Î¹ Î±Î½Î±Î¶Î®Ï„Î·ÏƒÎ· Î»Î­Î¾ÎµÏ‰Î½-ÎºÎ»ÎµÎ¹Î´Î¹ÏÎ½
    st.sidebar.header("Filters")
    category_filter = st.sidebar.selectbox("Select Category", ["All"] + list(site["categories"].keys()))
    keyword_search = st.sidebar.text_input("Search Keyword in Title", "")

    # Î•ÎºÏ„Î­Î»ÎµÏƒÎ· scraping ÎºÎ±Î¹ ÏƒÏ…Î»Î»Î¿Î³Î® Î´ÎµÎ´Î¿Î¼Î­Î½Ï‰Î½ Î³Î¹Î± ÎºÎ¬Î¸Îµ ÎºÎ±Ï„Î·Î³Î¿ÏÎ¯Î±
    for category, url in site["categories"].items():
        # Î§ÏÎ®ÏƒÎ· Ï„Î¿Ï… urlparse Î³Î¹Î± Î±ÏƒÏ†Î±Î»Î® ÎµÎ¾Î±Î³Ï‰Î³Î® Ï„Î¿Ï… base URL
        parsed_url = urlparse(url)
        base_url = f"{parsed_url.scheme}://{parsed_url.netloc}"
        
        articles = scrape_articles(url, selectors[category]["title"], selectors[category]["link"], base_url)
        for title, link in articles:
            content, meta_title, meta_description, meta_keywords = fetch_article_content(link)
            sentiment = analyze_sentiment(content)
            keyword_density, readability_scores = analyze_seo(content)
            data.append({
                "Category": category,
                "Title": title,
                "URL": link,
                "Sentiment": sentiment,
                "Meta Title": meta_title,
                "Meta Description": meta_description,
                "Meta Keywords": meta_keywords,
                "Keyword Density": keyword_density,
                "Flesch Reading Ease": readability_scores["flesch_reading_ease"],
                "Flesch-Kincaid Grade": readability_scores["flesch_kincaid_grade"],
                "Content": content[:500]  # Î ÏÎ¿ÎµÏ€Î¹ÏƒÎºÏŒÏ€Î·ÏƒÎ· Ï€ÎµÏÎ¹ÎµÏ‡Î¿Î¼Î­Î½Î¿Ï…
            })

    # Î”Î·Î¼Î¹Î¿Ï…ÏÎ³Î¯Î± DataFrame
    df = build_dataframe(data)

    # Î•Ï†Î±ÏÎ¼Î¿Î³Î® Ï†Î¯Î»Ï„ÏÏ‰Î½ Î²Î¬ÏƒÎµÎ¹ ÎºÎ±Ï„Î·Î³Î¿ÏÎ¯Î±Ï‚ ÎºÎ±Î¹ Î±Î½Î±Î¶Î®Ï„Î·ÏƒÎ·Ï‚ Î»Î­Î¾ÎµÏ‰Î½-ÎºÎ»ÎµÎ¹Î´Î¹ÏÎ½
    if category_filter != "All":
        df = df[df["Category"] == category_filter]
    if keyword_search:
        df = df[df["Title"].str.contains(keyword_search, case=False, na=False)]

    st.subheader(f"News Articles ({len(df)})")
    st.dataframe(df[["Category", "Title", "Sentiment", "Flesch Reading Ease", "Flesch-Kincaid Grade", "URL"]])

    # Î“ÏÎ¬Ï†Î·Î¼Î± ÎºÎ±Ï„Î±Î½Î¿Î¼Î®Ï‚ Ï„Î¿Ï… Sentiment
    st.subheader("ğŸ“ˆ Sentiment Distribution")
    if not df.empty:
        st.bar_chart(df["Sentiment"])
    else:
        st.write("No data available for the selected filters.")

    # Î›ÎµÏ€Ï„Î¿Î¼Î­ÏÎµÎ¹ÎµÏ‚ SEO Analysis Î³Î¹Î± ÎºÎ¬Î¸Îµ Î¬ÏÎ¸ÏÎ¿
    st.subheader("ğŸ” SEO Analysis Details")
    for index, row in df.iterrows():
        st.markdown(f"### {row['Title']}")
        st.write(f"**Category:** {row['Category']}")
        st.write(f"**Sentiment Score:** {row['Sentiment']}")
        st.write(f"**Flesch Reading Ease:** {row['Flesch Reading Ease']}")
        st.write(f"**Flesch-Kincaid Grade:** {row['Flesch-Kincaid Grade']}")
        st.write(f"**Meta Title:** {row['Meta Title']}")
        st.write(f"**Meta Description:** {row['Meta Description']}")
        st.write(f"**Meta Keywords:** {row['Meta Keywords']}")
        st.write(f"**Keyword Density:** {row['Keyword Density']}")
        st.write(f"**Content Preview:** {row['Content']}")
        if row["URL"]:
            st.markdown(f"[Read more]({row['URL']})")
        st.write("---")

    # Î”Ï…Î½Î±Ï„ÏŒÏ„Î·Ï„Î± ÎµÎ¾Î±Î³Ï‰Î³Î®Ï‚ Ï„Ï‰Î½ Î´ÎµÎ´Î¿Î¼Î­Î½Ï‰Î½ ÏƒÎµ CSV
    st.subheader("Download Data")
    csv_data = download_csv(df)
    st.download_button(
        label="Download CSV",
        data=csv_data,
        file_name='abc_news_data.csv',
        mime='text/csv'
    )

# ----------------------------
# Î•ÎºÏ„Î­Î»ÎµÏƒÎ· ÎºÏÎ´Î¹ÎºÎ± ÏŒÏ„Î±Î½ Ï„ÏÎ­Ï‡ÎµÎ¹ Ï‰Ï‚ ÎºÏÏÎ¹Î¿ Ï€ÏÏŒÎ³ÏÎ±Î¼Î¼Î±
# ----------------------------
if __name__ == '__main__':
    main()